{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c4148d",
   "metadata": {},
   "source": [
    "Final Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8560193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a30771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize method\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Grab Data, target,feature order\n",
    "class Dataset():\n",
    "    def __init__(self,targetFilename,featureFilename):\n",
    "        traintarget = np.loadtxt(targetFilename, delimiter=\" \")\n",
    "        trainfeature = np.loadtxt(featureFilename,delimiter=\" \")\n",
    "        self.X = torch.tensor(trainfeature, dtype=torch.float32)\n",
    "        self.y = torch.tensor(traintarget, dtype=torch.long)\n",
    "    # Retrieve X and y based on index\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    # Size of Dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "class TestDataset():\n",
    "    def __init__(self, featureFilename):\n",
    "        trainfeature = np.loadtxt(featureFilename,delimiter=\" \")\n",
    "        self.X = torch.tensor(trainfeature, dtype=torch.float32)\n",
    "    # Retrieve X based on index\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index]\n",
    "    # Size of Dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "# Get a random Subset of data\n",
    "def get_subset(X,y, subset_size):\n",
    "\n",
    "    rows = X[:, 0].to(dtype = torch.int)\n",
    "    cols = X[:, 1].to(dtype = torch.int)\n",
    "    vals = X[:, 2]\n",
    "\n",
    "    num_rows = rows.max() + 1\n",
    "    num_cols = cols.max() + 1\n",
    "\n",
    "    sparse_matrix = coo_matrix((vals, (rows, cols)), shape=(num_rows, num_cols))\n",
    "    X_np_full = sparse_matrix.toarray()\n",
    "    y_np_full = y.numpy()\n",
    "\n",
    "\n",
    "    indices = torch.randperm(sparse_matrix.shape[0])[:subset_size]\n",
    "    featureSubset = X_np_full[indices]\n",
    "    targetSubset = y_np_full[indices]\n",
    "    return featureSubset,targetSubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d4760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = Dataset(\"data/task5_beer/train.CT\",\"data/task5_beer/train.sparseX\")\n",
    "dev = Dataset(\"data/task5_beer/dev.CT\",\"data/task5_beer/dev.sparseX\")\n",
    "total_rows = int(train.X[:, 0].max().item()) + 1\n",
    "X_np,y_np = get_subset(train.X,train.y, total_rows)\n",
    "\n",
    "# scaler.fit(X_np)\n",
    "# joblib.dump(scaler, \"scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d439d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 256\n",
    "input_dim = X_np.shape[1]\n",
    "num_classes = len(np.unique(y_np))\n",
    "epochs = 18\n",
    "lr = 0.001\n",
    "batch_size = 256\n",
    "shuffle = True\n",
    "optimizer = torch.optim.Adam\n",
    "criterion = torch.nn.NLLLoss\n",
    "\n",
    "# Deep Neural Network skorch\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(DNN, self).__init__()\n",
    "        self.firstL = nn.Linear(input_dim, hidden_units)\n",
    "        self.hiddenL = nn.Linear(hidden_units, hidden_units)\n",
    "        self.hiddenL2 = nn.Linear(hidden_units, hidden_units)\n",
    "        # self.hiddenL3 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.outL = nn.Linear(hidden_units, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = Func.relu(self.firstL(x))\n",
    "        x = Func.relu(self.hiddenL(x))\n",
    "        x = Func.relu(self.hiddenL2(x))\n",
    "        # x = Func.relu(self.hiddenL3(x))\n",
    "        return Func.softmax(self.outL(x), dim=-1)\n",
    "\n",
    "network = NeuralNetClassifier(DNN,\n",
    "                              module__input_dim = input_dim,\n",
    "                              module__num_classes = num_classes,\n",
    "                              max_epochs = epochs,\n",
    "                              lr = lr,\n",
    "                              iterator_train__shuffle = shuffle,\n",
    "                              batch_size = batch_size,\n",
    "                              optimizer = optimizer,\n",
    "                              criterion = criterion,\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Ml-Final_project\",\n",
    "    config={\n",
    "        \"model_type\": \"Deep Neural Network\",\n",
    "        \"hidden_units\": hidden_units,\n",
    "        \"input_dim\": input_dim,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"shuffle\": shuffle,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"criterion\": criterion,\n",
    "        \"hidden_layers\": 2\n",
    "    }\n",
    ")\n",
    "\n",
    "# train\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "X_np_scaled = scaler.transform(X_np)\n",
    "network.fit(X_np_scaled, y_np)\n",
    "\n",
    "network.save_params(f_params='dnn_model.pt')\n",
    "\n",
    "# Dev\n",
    "# Convert dev sparse matrix to dense for sklearn\n",
    "dev_rows = dev.X[:, 0].to(dtype=torch.int)\n",
    "dev_cols = dev.X[:, 1].to(dtype=torch.int)\n",
    "dev_vals = dev.X[:, 2]\n",
    "\n",
    "num_rows = dev_rows.max() + 1\n",
    "num_cols = int(train.X[:,1].max().item()) + 1 \n",
    "\n",
    "\n",
    "dev_sparse_matrix = coo_matrix((dev_vals, (dev_rows, dev_cols)), shape=(num_rows, num_cols))\n",
    "X_dev = dev_sparse_matrix.toarray()\n",
    "X_dev = scaler.transform(X_dev)\n",
    "\n",
    "y_dev = dev.y.numpy()\n",
    "\n",
    "y_dev_pred = network.predict_proba(X_dev)\n",
    "y_dev_labels = network.predict(X_dev)\n",
    "\n",
    "dev_acc = accuracy_score(y_dev, y_dev_labels)\n",
    "dev_loss = log_loss(y_dev, y_dev_pred)\n",
    "\n",
    "recall = recall_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "precision = precision_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "f1 = f1_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "\n",
    "\n",
    "wandb.log({\"loss\": dev_loss, \"accuracy\": dev_acc, \"recall\": recall, \"precision\": precision, \"f1-score\": f1})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6ff43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Ml-Final_project\",\n",
    "    config={\n",
    "        \"model_type\": \"Random_Forests\",\n",
    "        \"number_of_forests\": 1,\n",
    "        \"n_estimators\": 100, \n",
    "        \"random_state\": 4,\n",
    "        # \"max_depth\": 15,\n",
    "        \"bootstrap\": 'True',\n",
    "        \"oob_score\": 'True'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train random forest\n",
    "rf1 = RandomForestClassifier(n_estimators=100, random_state=4, bootstrap=True, oob_score=True)\n",
    "\n",
    "# train\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "X_np_scaled = scaler.transform(X_np)\n",
    "rf1.fit(X_np_scaled, y_np)\n",
    "\n",
    "joblib.dump(rf1, 'randomForest.pkl', compress=('zlib', 9))\n",
    "\n",
    "# Dev\n",
    "# Convert dev sparse matrix to dense for sklearn\n",
    "dev_rows = dev.X[:, 0].to(dtype=torch.int)\n",
    "dev_cols = dev.X[:, 1].to(dtype=torch.int)\n",
    "dev_vals = dev.X[:, 2]\n",
    "\n",
    "num_rows = dev_rows.max() + 1\n",
    "num_cols = int(train.X[:,1].max().item()) + 1 \n",
    "\n",
    "\n",
    "dev_sparse_matrix = coo_matrix((dev_vals, (dev_rows, dev_cols)), shape=(num_rows, num_cols))\n",
    "X_dev = dev_sparse_matrix.toarray()\n",
    "X_dev = scaler.transform(X_dev)\n",
    "\n",
    "y_dev = dev.y.numpy()\n",
    "\n",
    "y_dev_pred = rf1.predict_proba(X_dev)\n",
    "y_dev_labels = rf1.predict(X_dev)\n",
    "\n",
    "dev_acc = accuracy_score(y_dev, y_dev_labels)\n",
    "dev_loss = log_loss(y_dev, y_dev_pred)\n",
    "\n",
    "recall = recall_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "precision = precision_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "f1 = f1_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "\n",
    "\n",
    "wandb.log({\"loss\": dev_loss, \"accuracy\": dev_acc, \"recall\": recall, \"precision\": precision, \"f1-score\": f1})\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d972a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_init=0.001\n",
    "early_stopping = True\n",
    "alpha = 0.001\n",
    "max_iter=500\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Ml-Final_project\",\n",
    "    config={\n",
    "        \"model_type\": \"MLPClassifier\",\n",
    "        \"mlp\": [\n",
    "            {\"name\": \"mlp1\", \"hidden_layer_sizes\":(150, 150, 150, 150), \"activation\": 'relu', \"alpha\": alpha, \"learning_rate\": learning_rate_init,\n",
    "             \"early_stopping\": True, \"max_iter\": max_iter, \"random_state\": 1},\n",
    "            {\"name\": \"mlp2\", \"hidden_layer_sizes\":(100, 100, 100, 100), \"activation\": 'relu', \"alpha\": alpha, \"learning_rate\": learning_rate_init,\n",
    "             \"early_stopping\": True, \"max_iter\": max_iter, \"random_state\": 2},\n",
    "            {\"name\": \"mlp3\", \"hidden_layer_sizes\":(50, 50, 50, 50), \"activation\": 'relu', \"alpha\": alpha, \"learning_rate\": learning_rate_init,\n",
    "             \"early_stopping\": True, \"max_iter\": max_iter, \"random_state\": 3}\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes = (150,150,150,150), activation = 'relu'\n",
    "                       ,alpha = 0.001, learning_rate_init=0.001, early_stopping=True, max_iter=500, random_state=1)\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes = (100,100,100,100), activation = 'relu'\n",
    "                       ,alpha = 0.001, learning_rate_init=0.001, early_stopping=True, max_iter=500, random_state=2)\n",
    "mlp3 = MLPClassifier(hidden_layer_sizes = (50,50,50,50), activation = 'relu'\n",
    "                       ,alpha = 0.01, learning_rate_init=0.001, early_stopping=True, max_iter=500, random_state=3)\n",
    "\n",
    "ensemble2 = VotingClassifier(estimators=[\n",
    "                                        (\"mlp1\", mlp1),\n",
    "                                        (\"mlp2\", mlp2),\n",
    "                                        (\"mlp3\", mlp3),\n",
    "                                          ],\n",
    "                                          voting=\"soft\")\n",
    "\n",
    "# train\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "X_np_scaled = scaler.transform(X_np)\n",
    "ensemble2.fit(X_np_scaled, y_np)\n",
    "\n",
    "joblib.dump(ensemble2, 'mlp_ensemble.pkl')\n",
    "\n",
    "# Dev\n",
    "# Convert dev sparse matrix to dense for sklearn\n",
    "dev_rows = dev.X[:, 0].to(dtype=torch.int)\n",
    "dev_cols = dev.X[:, 1].to(dtype=torch.int)\n",
    "dev_vals = dev.X[:, 2]\n",
    "\n",
    "num_rows = dev_rows.max() + 1\n",
    "num_cols = int(train.X[:,1].max().item()) + 1 \n",
    "\n",
    "\n",
    "dev_sparse_matrix = coo_matrix((dev_vals, (dev_rows, dev_cols)), shape=(num_rows, num_cols))\n",
    "X_dev = dev_sparse_matrix.toarray()\n",
    "X_dev = scaler.transform(X_dev)\n",
    "\n",
    "y_dev = dev.y.numpy()\n",
    "\n",
    "y_dev_pred = ensemble2.predict_proba(X_dev)\n",
    "y_dev_labels = ensemble2.predict(X_dev)\n",
    "\n",
    "dev_acc = accuracy_score(y_dev, y_dev_labels)\n",
    "dev_loss = log_loss(y_dev, y_dev_pred)\n",
    "\n",
    "recall = recall_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "precision = precision_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "f1 = f1_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "\n",
    "\n",
    "wandb.log({\"loss\": dev_loss, \"accuracy\": dev_acc, \"recall\": recall, \"precision\": precision, \"f1-score\": f1})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "booster = 'gbtree'\n",
    "n_estimators = 400\n",
    "lr = 0.001\n",
    "max_depth = 7\n",
    "sub_sample = 0.8\n",
    "colsample_bytree = 0.8\n",
    "objective = 'multi:softprob'\n",
    "eval_metric = 'mlogloss'\n",
    "num_classes = len(np.unique(y_np))\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Ml-Final_project\",\n",
    "    config={\n",
    "        \"model_type\": \"XGBoost\",\n",
    "        \"booster\": booster,\n",
    "        \"lr\": lr,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"sub_sample\": sub_sample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"objective\": objective,\n",
    "        \"eval_metric\": eval_metric,\n",
    "        \"num_class\": num_classes,\n",
    "        \"n_estimators\": n_estimators\n",
    "    }\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'num_class': num_classes,\n",
    "    'n_estimators': n_estimators\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(**params)\n",
    "\n",
    "# train\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "X_np_scaled = scaler.transform(X_np)\n",
    "xgb.fit(X_np_scaled, y_np)\n",
    "\n",
    "joblib.dump(xgb, 'xgboost.pkl')\n",
    "\n",
    "# Dev\n",
    "# Convert dev sparse matrix to dense for sklearn\n",
    "dev_rows = dev.X[:, 0].to(dtype=torch.int)\n",
    "dev_cols = dev.X[:, 1].to(dtype=torch.int)\n",
    "dev_vals = dev.X[:, 2]\n",
    "\n",
    "num_rows = dev_rows.max() + 1\n",
    "num_cols = int(train.X[:,1].max().item()) + 1 \n",
    "\n",
    "\n",
    "dev_sparse_matrix = coo_matrix((dev_vals, (dev_rows, dev_cols)), shape=(num_rows, num_cols))\n",
    "X_dev = dev_sparse_matrix.toarray()\n",
    "X_dev = scaler.transform(X_dev)\n",
    "\n",
    "y_dev = dev.y.numpy()\n",
    "\n",
    "y_dev_pred = xgb.predict_proba(X_dev)\n",
    "y_dev_labels = xgb.predict(X_dev)\n",
    "\n",
    "dev_acc = accuracy_score(y_dev, y_dev_labels)\n",
    "dev_loss = log_loss(y_dev, y_dev_pred)\n",
    "\n",
    "recall = recall_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "precision = precision_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "f1 = f1_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "\n",
    "\n",
    "wandb.log({\"loss\": dev_loss, \"accuracy\": dev_acc, \"recall\": recall, \"precision\": precision, \"f1-score\": f1})\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5206a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembling\n",
    "rf_trained = joblib.load('randomForest.pkl')\n",
    "xgb_trained = joblib.load('xgboost.pkl')\n",
    "mlp_ensemble_trained = joblib.load('mlp_ensemble.pkl')\n",
    "\n",
    "net_trained = NeuralNetClassifier(DNN,\n",
    "                                module__input_dim = input_dim,\n",
    "                                module__num_classes = num_classes,\n",
    "                                max_epochs = epochs,\n",
    "                                lr = lr,\n",
    "                                iterator_train__shuffle = shuffle,\n",
    "                                batch_size = batch_size,\n",
    "                                optimizer = optimizer,\n",
    "                                criterion = criterion\n",
    "                                )\n",
    "net_trained.initialize()\n",
    "net_trained.load_params(f_params=\"dnn_model.pt\")\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Ml-Final_project\",\n",
    "    config={\n",
    "        \"model_type\": \"Ensembling\",\n",
    "        \"xgboost\": \"XGBoost\",\n",
    "        \"mlp\": \"MLPClassifier\",\n",
    "        \"rf\": \"RandomForestClassifier\",\n",
    "        \"dnn\": \"NeuralNetClassifier\"\n",
    "\n",
    "    }\n",
    ")\n",
    "\n",
    "# Dev\n",
    "# Convert dev sparse matrix to dense for sklearn\n",
    "dev_rows = dev.X[:, 0].to(dtype=torch.int)\n",
    "dev_cols = dev.X[:, 1].to(dtype=torch.int)\n",
    "dev_vals = dev.X[:, 2]\n",
    "\n",
    "num_rows = dev_rows.max() + 1\n",
    "num_cols = int(train.X[:,1].max().item()) + 1 \n",
    "\n",
    "\n",
    "dev_sparse_matrix = coo_matrix((dev_vals, (dev_rows, dev_cols)), shape=(num_rows, num_cols))\n",
    "X_dev = dev_sparse_matrix.toarray()\n",
    "\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "X_dev = scaler.transform(X_dev)\n",
    "\n",
    "xgb_probs = xgb_trained.predict_proba(X_dev)\n",
    "mlp_probs = mlp_ensemble_trained.predict_proba(X_dev)\n",
    "rf_probs = rf_trained.predict_proba(X_dev)\n",
    "dnn_probs = net_trained.predict_proba(X_dev)\n",
    "\n",
    "ensemble = (\n",
    "    5*rf_probs + \n",
    "    2*xgb_probs +\n",
    "    2*mlp_probs +\n",
    "    1*dnn_probs\n",
    ") / 10\n",
    "\n",
    "y_dev = dev.y.numpy()\n",
    "\n",
    "y_dev_pred = ensemble\n",
    "y_dev_labels = np.argmax(ensemble, axis=1)\n",
    "\n",
    "dev_acc = accuracy_score(y_dev, y_dev_labels)\n",
    "dev_loss = log_loss(y_dev, y_dev_pred)\n",
    "\n",
    "recall = recall_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "precision = precision_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "f1 = f1_score(y_dev, y_dev_labels, average = \"macro\")\n",
    "\n",
    "\n",
    "wandb.log({\"loss\": dev_loss, \"accuracy\": dev_acc, \"recall\": recall, \"precision\": precision, \"f1-score\": f1})\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9982b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart\n",
    "models = ['DNN', 'Random Forest', 'XGBoost', 'MLP', 'Ensemble']\n",
    "accuracy = [0.373, 0.427, 0.408, 0.402, 0.469]\n",
    "f1_score = [0.273, 0.328, 0.306, 0.285, 0.371]\n",
    "loss = [2.809, 3.876, 2.128, 2.208, 1.957]\n",
    "precision = [0.299, 0.507, 0.410, 0.358, 0.531]\n",
    "recall = [0.267, 0.278, 0.276, 0.269, 0.326]\n",
    "\n",
    "# Convert accuracy-like metrics to percentages\n",
    "accuracy = [x * 100 for x in accuracy]\n",
    "f1_score = [x * 100 for x in f1_score]\n",
    "precision = [x * 100 for x in precision]\n",
    "recall = [x * 100 for x in recall]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.15\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Bar chart for accuracy, precision, recall, f1\n",
    "ax1.bar(x - 1.5*width, accuracy, width, label='Accuracy')\n",
    "ax1.bar(x + 1.5*width, f1_score, width, label='F1 Score')\n",
    "ax1.bar(x - 0.5*width, precision, width, label='Precision')\n",
    "ax1.bar(x + 0.5*width, recall, width, label='Recall')\n",
    "\n",
    "ax1.set_ylabel('Score (%)')\n",
    "ax1.set_xlabel('Models')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models)\n",
    "ax1.set_title('Model Comparison on Dev Set')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Secondary axis for loss\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x, loss, 'o--', color='black', label='Loss')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Gets top 10\n",
    "top_k = 10\n",
    "top_labels = [label for label, _ in Counter(y_dev).most_common(top_k)]\n",
    "mask = [y in top_labels for y in y_dev]\n",
    "\n",
    "filtered_true = np.array(y_dev)[mask]\n",
    "filtered_pred = np.array(y_dev_labels)[mask]\n",
    "filtered_labels = sorted(set(top_labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(filtered_true, filtered_pred, labels = filtered_labels, normalize='true')\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=filtered_labels)\n",
    "display.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ae3dcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (22860, 10355)\n",
      "rf_probs: (22860, 80)\n",
      "xgb_probs: (22860, 80)\n",
      "mlp_probs: (22860, 80)\n",
      "dnn_probs: (22860, 80)\n",
      "y_labels shape: (22860,)\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "rf_trained = joblib.load('randomForest.pkl')\n",
    "xgb_trained = joblib.load('xgboost.pkl')\n",
    "mlp_ensemble_trained = joblib.load('mlp_ensemble.pkl')\n",
    "\n",
    "net_trained = NeuralNetClassifier(DNN,\n",
    "                                module__input_dim = input_dim,\n",
    "                                module__num_classes = num_classes,\n",
    "                                max_epochs = epochs,\n",
    "                                lr = lr,\n",
    "                                iterator_train__shuffle = shuffle,\n",
    "                                batch_size = batch_size,\n",
    "                                optimizer = optimizer,\n",
    "                                criterion = criterion\n",
    "                                )\n",
    "net_trained.initialize()\n",
    "net_trained.load_params(f_params=\"dnn_model.pt\")\n",
    "\n",
    "test = TestDataset(\"test_features/task5/test.sparseX\")\n",
    "# Test\n",
    "# Convert test sparse matrix to dense for sklearn\n",
    "test_rows = test.X[:, 0].to(dtype=torch.int)\n",
    "test_cols = test.X[:, 1].to(dtype=torch.int)\n",
    "test_vals = test.X[:, 2]\n",
    "\n",
    "num_rows = len(torch.unique(test_rows))\n",
    "num_cols = int(test.X[:,1].max().item()) + 1 \n",
    "\n",
    "\n",
    "test_sparse_matrix = coo_matrix((test_vals, (test_rows, test_cols)), shape=(num_rows, num_cols))\n",
    "X_test = test_sparse_matrix.toarray()\n",
    "\n",
    "scaler.fit(X_np)\n",
    "n_features = scaler.n_features_in_\n",
    "if X_test.shape[1] < n_features:\n",
    "    pad_width = n_features - X_test.shape[1]\n",
    "    X_test = np.pad(X_test, ((0, 0), (0, pad_width)), mode='constant')\n",
    "elif X_test.shape[1] > n_features:\n",
    "    X_test = X_test[:, :n_features]\n",
    "\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "xgb_probs = xgb_trained.predict_proba(X_test)\n",
    "mlp_probs = mlp_ensemble_trained.predict_proba(X_test)\n",
    "rf_probs = rf_trained.predict_proba(X_test)\n",
    "dnn_probs = net_trained.predict_proba(X_test)\n",
    "\n",
    "ensemble = (\n",
    "    5*rf_probs + \n",
    "    2*xgb_probs +\n",
    "    2*mlp_probs +\n",
    "    1*dnn_probs\n",
    ") / 10\n",
    "\n",
    "\n",
    "y_pred = ensemble\n",
    "y_labels = np.argmax(ensemble, axis=1)\n",
    "\n",
    "submission = pd.DataFrame({\"Label\": y_labels})\n",
    "submission.to_csv(\"task5.predictions\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
